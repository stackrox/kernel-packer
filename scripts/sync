#!/usr/bin/env bash

DIR="$(cd "$(dirname "$0")" && pwd)"

info() {
    echo "[INFO] $*" 1>&2
}

main() {
    # Directory that holds the crawled url text files.
    #
    # Example: "./kernel-package-lists"
    package_list_dir="$1"

    # GCS bucket containing uploaded files.
    #
    # Example: "gs://stackrox-kernel-packages"
    bucket_name_list="$2"
    IFS=',' read -r -a bucket_names <<< "$bucket_name_list"

    # Directory to download urls into.
    #
    # Example: ".build-data/downloads"
    download_dir="$3"
    mkdir -p "$download_dir"

    info 'Generating a list of crawled urls.'
    all_crawled_packages="$(./scripts/crawled-inventory "$package_list_dir")"
    echo "$all_crawled_packages" > all-crawled-packages.log
    echo "Found $(wc -l all-crawled-packages.log) crawled packages"

    info 'Generating a list of bucket files.'
    all_bucket_files="$(./scripts/package-inventory "$bucket_name_list")"
    echo "$all_bucket_files" > all-bucket-files.log
    echo "Found $(wc -l all-bucket-files.log) bucket files"

    info 'Downloading missing files from origin source.'
    while read -r url; do
        # Simplify url into the naming scheme used in the bucket.
        filename="$(simplify "$url")"

        # Check if this specific file exists in the bucket listing.
        if ! echo "$all_bucket_files" 2>/dev/null | contains_exact "$filename"; then
            echo "Downloading $url and uploading to bucket"

            # This file didn't exist in the bucket listing, so download it.
            download "$url" "${download_dir}/${filename}"

            # Don't bother trying to upload if the download failed
            if [[ -f "${download_dir}/${filename}" ]]; then
                # Now upload and delete to minimise file system use
                gsutil -m cp "${download_dir}/${filename}" "${bucket_names[0]%/}/"

                # This is maaybe a cheap trick to keep a record of
                # the downloaded file without incurring the file system
                # hit
                echo > "${download_dir}/${filename}"
            fi
        fi
    done <<< "$all_crawled_packages"
    echo

    info 'Sanity checking downloaded files.'
    echo "Downloaded $(find "${download_dir}" -size 0 -delete -print | wc -l) files"
    echo
}

contains_exact() {
    grep -Eq "^$1$"
}

contains() {
    grep -qF "$1"
}

get_rhsm_api_access_token() {
    token_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
    curl $token_url -d grant_type=refresh_token -d client_id=rhsm-api -d refresh_token="$RHSM_OFFLINE_TOKEN" | jq -r '.access_token'
}

download_using_rhsm_api() {
	url=$1
	file=$2

	token="$(get_rhsm_api_access_token)"
	url_info="$(curl -H "Authorization: Bearer $token" "$url" -H "accept: application/json")"
	temp_url="$(echo "$url_info" | jq -r .body.href)"
	curl -H "Authorization: Bearer $token" "$temp_url" --output "$file" || rm -f "$file"
}

download() {
    url="$1"
    file="$2"
    if echo "$url" | contains 'cdn.redhat.com'; then
        wget \
            --no-check-certificate \
            --certificate .build-data/rhel-certs/rhel-cert.pem \
            --private-key .build-data/rhel-certs/rhel-key.pem \
            -nv "$url" -O "$file" || rm -f "$file"
    elif echo "$url" | contains 'api.access.redhat.com'; then
        download_using_rhsm_api "$url" "$file"
    elif echo "$url" | contains 'docker.io'; then
        wget \
            --header "$("${DIR}/docker-desktop/auth-header.sh")" \
            -nv "$url" -O "$file" || rm -f "$file"
    elif echo "$url" | contains 'updates.suse.com'; then
        base_url="$(echo "$url" | rev | cut -d'/' -f3- | rev)/"
        tokens_file="${DIR}/../.build-data/suse-repo-tokens/repos.json"
        token="$(jq -r --arg BASE_URL "${base_url}" '.[]|select(.url|contains($BASE_URL))|.token' "${tokens_file}")"
        wget -nv "${url}?${token}" -O "$file" || rm -f "$file"
    elif echo "$url" | contains 'esm.ubuntu.com/infra/'; then
        wget -nv --user "bearer" --password "$UBUNTU_ESM_INFRA_BEARER_TOKEN" "$url" -O "$file" || rm -f "$file"
    elif echo "$url" | contains 'esm.ubuntu.com/fips/'; then
        wget -nv --user "bearer" --password "$UBUNTU_FIPS_BEARER_TOKEN" "$url" -O "$file" || rm -f "$file"
    elif echo "$url" | contains 'esm.ubuntu.com/fips-updates/'; then
        wget -nv --user "bearer" --password "$UBUNTU_FIPS_UPDATES_BEARER_TOKEN" "$url" -O "$file" || rm -f "$file"
    else
        wget -nv "$url" -O "$file" || rm -f "$file"
    fi
}

simplify() {
    echo "$1" | tr -c 'a-zA-Z0-9_.\n' '-'
}

main "$@"
