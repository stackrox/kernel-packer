name: Repackage Bundles

on: workflow_call

jobs:
  split-tasks:
    runs-on: ubuntu-latest
    env:
      BUILD_DATA_DIR: ./build-data
    outputs:
      parallel-jobs-number: ${{ steps.split-packages.outputs.parallel-jobs-number }}
      parallel-array: ${{ steps.split-packages.outputs.parallel-array }}
    steps:
      - uses: actions/checkout@v3
      - uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GOOGLE_CREDENTIALS_KERNEL_CACHE }}'

      - uses: google-github-actions/setup-gcloud@v1
      
      - uses: ./.github/actions/env

      - name: Prepare Cache
        run: |
          mkdir -p "${BUILD_DATA_DIR}/cache"
          IFS=',' read -r -a bucket_names <<< "${KERNEL_BUNDLE_BUCKET}"
          for bucket in "${bucket_names[@]}"; do
            if [ ! -f "${BUILD_DATA_DIR}/cache/cache.yml" ]; then
              gsutil cp "${bucket}/cache.yml" "${BUILD_DATA_DIR}/cache/cache.yml" || true
            fi
          done
          touch "${BUILD_DATA_DIR}/cache/cache.yml"

      - name: List packages
        run: make list-files

      - name: Archive package list
        uses: actions/upload-artifact@v3
        with:
          name: package-list
          path: .build-data/packages.txt

      - name: Split package file
        id: split-packages
        run: |
          import math
          import os
          import json

          def chunks(data, chunk_size):
            c = []
            for i in range(chunk_size):
              c.append(data[i::chunk_size])
            return c

          packages = []
          with open('.build-data/packages.txt') as pkg:
            packages = pkg.readlines()

          parallel_jobs = math.ceil(len(packages) / 500)
          if parallel_jobs > 32:
            parallel_jobs = 32

          parallel_jobs = 1

          jobs = chunks(packages, parallel_jobs)
          parallel = list(range(parallel_jobs))

          combined = {id: jobs[id] for id in range(parallel_jobs)}

          with open(os.environ['GITHUB_OUTPUT'], 'w') as output:
            output.write(f'parallel-jobs-number={parallel_jobs}\n')
            output.write(f'parallel-array={json.dumps(parallel)}\n')

          with open('jobs.json', 'w') as j:
            j.write(json.dumps(combined))

        shell: python

      - name: Archive jobs
        uses: actions/upload-artifact@v3
        with:
          name: package-jobs
          path: jobs.json

  repackage:
    runs-on: ubuntu-latest
    needs: split-tasks
    if: ${{ needs.split-tasks.outputs.parallel-jobs-number > 0 }}
    strategy:
      matrix:
        packer: ${{ fromJSON(needs.split-tasks.outputs.parallel-array) }}
    env:
      MANIFEST_FILE: ./kernel-package-lists/manifest.yml
      BUILD_DATA_DIR: ./build-data
    steps:
      - uses: actions/checkout@v3
      - uses: google-github-actions/auth@v1
        with:
          credentials_json: '${{ secrets.GOOGLE_CREDENTIALS_KERNEL_CACHE }}'

      - uses: google-github-actions/setup-gcloud@v1

      - uses: ./.github/actions/env

      - name: Build packers
        run: make packers

      - name: Refresh manifest
        run: make manifest

      - name: Download Cache
        run: |
          mkdir -p "${BUILD_DATA_DIR}/cache"
          IFS=',' read -r -a bucket_names <<< "${KERNEL_BUNDLE_BUCKET}"
          for bucket in "${bucket_names[@]}"; do
            if [ ! -f "${BUILD_DATA_DIR}/cache/cache.yml" ]; then
              gsutil cp "${bucket}/cache.yml" "${BUILD_DATA_DIR}/cache/cache.yml" || true
            fi
          done
          touch "${BUILD_DATA_DIR}/cache/cache.yml"

      - name: Download jobs
        uses: actions/download-artifact@v3
        with:
          name: package-jobs

      - name: Write package file
        run: |
          import os
          import json

          with open('jobs.json') as jobs:
            packages = json.load(jobs)

          try:
            os.mkdir('.build-data')
          except OSError:
            pass

          with open('.build-data/packages.txt', 'w') as output:
            for package in packages[${{ matrix.packer }}]:
              output.write(package)
        shell: python

      - name: Repackage
        run: |
          mkdir -p "${BUILD_DATA_DIR}/packages"

          # split into 100 line chunks, to avoid overloading the node
          # with huge amounts of package downloads
          split -l 100 --numeric-suffixes "${BUILD_DATA_DIR}/packages.txt" "${BUILD_DATA_DIR}/chunk"
          
          for chunk in .build-data/chunk*; do
            ./scripts/download-packages "${BUILD_DATA_DIR}" "${KERNEL_PACKAGE_BUCKET}" "${chunk}"
            
            go run ./tools/repackage-kernels/main.go \
                -manifest "${MANIFEST_FILE}" \
                -cache-dir "${BUILD_DATA_DIR}/cache" \
                -pkg-dir "${BUILD_DATA_DIR}/packages" \
                -bundle-dir "${BUILD_DATA_DIR}/bundles" \
                -action build

            ./scripts/upload-bundles "${BUILD_DATA_DIR}" "$KERNEL_BUNDLE_BUCKET"

            # clean up repackaged bundles and packages
            rm -rf "${BUILD_DATA_DIR}/packages/*"
            rm -rf "${BUILD_DATA_DIR}/bundles/*"
          done

      - name: Cache
        run: make combine-cache clean-cache

      - name: Upload cache
        run: |
          IFS=',' read -r -a bucket_names <<< "${KERNEL_BUNDLE_BUCKET}"
          gsutil cp .build-data/cache/cache.yml "${bucket_names[0]}/cache.yml"

      - name: Commit to Collector Repo
        if: ${{ github.event_name != 'pull_request' }}
        run: make robo-collector-commit
